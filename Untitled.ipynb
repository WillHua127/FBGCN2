{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu pretrained/affb2bbb1ed44119aeac6fbbad3707b7.pt\n",
      "c: 0.38572952151298523\n",
      "c_high: 0.5035539865493774\n",
      "Epoch:0001 train loss:1.095 acc:39.06 | val loss:1.087 acc:39.78\n",
      "c: 0.3935306668281555\n",
      "c_high: 0.4957475960254669\n",
      "Epoch:0002 train loss:1.087 acc:39.19 | val loss:1.079 acc:39.78\n",
      "c: 0.4019363224506378\n",
      "c_high: 0.4873749613761902\n",
      "Epoch:0003 train loss:1.079 acc:41.50 | val loss:1.070 acc:43.79\n",
      "c: 0.41072970628738403\n",
      "c_high: 0.47864091396331787\n",
      "Epoch:0004 train loss:1.071 acc:46.10 | val loss:1.060 acc:52.69\n",
      "c: 0.4197951853275299\n",
      "c_high: 0.469658225774765\n",
      "Epoch:0005 train loss:1.061 acc:49.95 | val loss:1.049 acc:60.02\n",
      "c: 0.42910662293434143\n",
      "c_high: 0.4604451060295105\n",
      "Epoch:0006 train loss:1.050 acc:52.14 | val loss:1.038 acc:63.99\n",
      "c: 0.43862003087997437\n",
      "c_high: 0.45104628801345825\n",
      "Epoch:0007 train loss:1.039 acc:53.19 | val loss:1.026 acc:64.98\n",
      "c: 0.44835811853408813\n",
      "c_high: 0.44141101837158203\n",
      "Epoch:0008 train loss:1.028 acc:53.59 | val loss:1.015 acc:63.63\n",
      "c: 0.4582884609699249\n",
      "c_high: 0.4315643310546875\n",
      "Epoch:0009 train loss:1.017 acc:54.38 | val loss:1.003 acc:61.95\n",
      "c: 0.46835359930992126\n",
      "c_high: 0.42153745889663696\n",
      "Epoch:0010 train loss:1.006 acc:54.29 | val loss:0.992 acc:61.36\n",
      "c: 0.4785049557685852\n",
      "c_high: 0.41136646270751953\n",
      "Epoch:0011 train loss:0.994 acc:54.51 | val loss:0.979 acc:61.70\n",
      "c: 0.4887583553791046\n",
      "c_high: 0.4010449945926666\n",
      "Epoch:0012 train loss:0.983 acc:54.55 | val loss:0.964 acc:63.11\n",
      "c: 0.4991682171821594\n",
      "c_high: 0.3905693590641022\n",
      "Epoch:0013 train loss:0.970 acc:55.71 | val loss:0.945 acc:64.41\n",
      "c: 0.5097151398658752\n",
      "c_high: 0.37994179129600525\n",
      "Epoch:0014 train loss:0.953 acc:56.96 | val loss:0.925 acc:65.04\n",
      "c: 0.5203996300697327\n",
      "c_high: 0.36932286620140076\n",
      "Epoch:0015 train loss:0.933 acc:58.04 | val loss:0.902 acc:65.47\n",
      "c: 0.531173586845398\n",
      "c_high: 0.35875922441482544\n",
      "Epoch:0016 train loss:0.912 acc:60.39 | val loss:0.878 acc:66.91\n",
      "c: 0.5420241951942444\n",
      "c_high: 0.3482182025909424\n",
      "Epoch:0017 train loss:0.889 acc:62.58 | val loss:0.852 acc:71.08\n",
      "c: 0.55298912525177\n",
      "c_high: 0.33760520815849304\n",
      "Epoch:0018 train loss:0.868 acc:64.67 | val loss:0.826 acc:75.31\n",
      "c: 0.5641042590141296\n",
      "c_high: 0.32685014605522156\n",
      "Epoch:0019 train loss:0.844 acc:68.38 | val loss:0.798 acc:77.65\n",
      "c: 0.5753874182701111\n",
      "c_high: 0.315920889377594\n",
      "Epoch:0020 train loss:0.819 acc:70.91 | val loss:0.768 acc:79.27\n",
      "c: 0.5868495106697083\n",
      "c_high: 0.3047959804534912\n",
      "Epoch:0021 train loss:0.791 acc:72.46 | val loss:0.733 acc:79.90\n",
      "c: 0.5984957814216614\n",
      "c_high: 0.29344698786735535\n",
      "Epoch:0022 train loss:0.764 acc:72.38 | val loss:0.694 acc:80.36\n",
      "c: 0.6103183031082153\n",
      "c_high: 0.2818774878978729\n",
      "Epoch:0023 train loss:0.732 acc:73.35 | val loss:0.650 acc:80.82\n",
      "c: 0.6222933530807495\n",
      "c_high: 0.27008679509162903\n",
      "Epoch:0024 train loss:0.694 acc:74.59 | val loss:0.597 acc:81.49\n",
      "c: 0.6343625783920288\n",
      "c_high: 0.25809577107429504\n",
      "Epoch:0025 train loss:0.653 acc:74.78 | val loss:0.529 acc:81.77\n",
      "c: 0.6463981866836548\n",
      "c_high: 0.24598397314548492\n",
      "Epoch:0026 train loss:0.609 acc:75.81 | val loss:0.465 acc:81.57\n",
      "c: 0.6554907560348511\n",
      "c_high: 0.2376219928264618\n",
      "Epoch:0027 train loss:0.658 acc:74.98 | val loss:0.464 acc:82.49\n",
      "c: 0.664065957069397\n",
      "c_high: 0.22960352897644043\n",
      "Epoch:0028 train loss:0.581 acc:76.58 | val loss:0.476 acc:82.73\n",
      "c: 0.6726686954498291\n",
      "c_high: 0.22121231257915497\n",
      "Epoch:0029 train loss:0.563 acc:78.01 | val loss:0.480 acc:82.82\n",
      "c: 0.6814027428627014\n",
      "c_high: 0.2123275101184845\n",
      "Epoch:0030 train loss:0.560 acc:78.46 | val loss:0.476 acc:83.23\n",
      "c: 0.6902757287025452\n",
      "c_high: 0.2029516100883484\n",
      "Epoch:0031 train loss:0.550 acc:79.28 | val loss:0.469 acc:83.55\n",
      "c: 0.6992239952087402\n",
      "c_high: 0.19316834211349487\n",
      "Epoch:0032 train loss:0.544 acc:79.17 | val loss:0.461 acc:83.93\n",
      "c: 0.7081557512283325\n",
      "c_high: 0.18308982253074646\n",
      "Epoch:0033 train loss:0.540 acc:79.92 | val loss:0.453 acc:84.34\n",
      "c: 0.7170138955116272\n",
      "c_high: 0.17278152704238892\n",
      "Epoch:0034 train loss:0.531 acc:80.15 | val loss:0.444 acc:84.45\n",
      "c: 0.7256842255592346\n",
      "c_high: 0.1623881757259369\n",
      "Epoch:0035 train loss:0.521 acc:80.63 | val loss:0.434 acc:84.53\n",
      "c: 0.7341715097427368\n",
      "c_high: 0.15188683569431305\n",
      "Epoch:0036 train loss:0.506 acc:80.84 | val loss:0.424 acc:84.64\n",
      "c: 0.7424607276916504\n",
      "c_high: 0.14128375053405762\n",
      "Epoch:0037 train loss:0.496 acc:81.25 | val loss:0.416 acc:84.58\n",
      "c: 0.7504830360412598\n",
      "c_high: 0.13067543506622314\n",
      "Epoch:0038 train loss:0.495 acc:81.03 | val loss:0.408 acc:84.63\n",
      "c: 0.7582300901412964\n",
      "c_high: 0.12005915492773056\n",
      "Epoch:0039 train loss:0.490 acc:81.00 | val loss:0.402 acc:84.71\n",
      "c: 0.7655867338180542\n",
      "c_high: 0.10965622961521149\n",
      "Epoch:0040 train loss:0.482 acc:81.59 | val loss:0.394 acc:84.91\n",
      "c: 0.7724204659461975\n",
      "c_high: 0.09980249404907227\n",
      "Epoch:0041 train loss:0.477 acc:82.09 | val loss:0.387 acc:85.06\n",
      "c: 0.7787407636642456\n",
      "c_high: 0.09051863104104996\n",
      "Epoch:0042 train loss:0.466 acc:82.27 | val loss:0.381 acc:85.15\n",
      "c: 0.7845264077186584\n",
      "c_high: 0.08194673806428909\n",
      "Epoch:0043 train loss:0.461 acc:82.70 | val loss:0.377 acc:85.40\n",
      "c: 0.7897046804428101\n",
      "c_high: 0.07449031621217728\n",
      "Epoch:0044 train loss:0.471 acc:82.32 | val loss:0.374 acc:85.52\n",
      "c: 0.7943188548088074\n",
      "c_high: 0.06813526153564453\n",
      "Epoch:0045 train loss:0.456 acc:82.98 | val loss:0.372 acc:85.58\n",
      "c: 0.7983987331390381\n",
      "c_high: 0.06292647868394852\n",
      "Epoch:0046 train loss:0.456 acc:83.32 | val loss:0.370 acc:85.74\n",
      "c: 0.8019923567771912\n",
      "c_high: 0.058805886656045914\n",
      "Epoch:0047 train loss:0.448 acc:83.26 | val loss:0.368 acc:85.80\n",
      "c: 0.8051385283470154\n",
      "c_high: 0.055743180215358734\n",
      "Epoch:0048 train loss:0.449 acc:83.54 | val loss:0.367 acc:85.83\n",
      "c: 0.8079273700714111\n",
      "c_high: 0.05337807163596153\n",
      "Epoch:0049 train loss:0.431 acc:83.75 | val loss:0.365 acc:85.91\n",
      "c: 0.8104261755943298\n",
      "c_high: 0.05144808441400528\n",
      "Epoch:0050 train loss:0.437 acc:83.34 | val loss:0.363 acc:86.04\n",
      "c: 0.8126614093780518\n",
      "c_high: 0.04992019757628441\n",
      "Epoch:0051 train loss:0.442 acc:83.85 | val loss:0.360 acc:86.16\n",
      "c: 0.8146892786026001\n",
      "c_high: 0.04855073243379593\n",
      "Epoch:0052 train loss:0.428 acc:84.00 | val loss:0.359 acc:86.51\n",
      "c: 0.816541850566864\n",
      "c_high: 0.04723493754863739\n",
      "Epoch:0053 train loss:0.425 acc:83.92 | val loss:0.357 acc:86.62\n",
      "c: 0.8182505965232849\n",
      "c_high: 0.04585662856698036\n",
      "Epoch:0054 train loss:0.423 acc:84.42 | val loss:0.356 acc:86.69\n",
      "c: 0.8198150992393494\n",
      "c_high: 0.044512100517749786\n",
      "Epoch:0055 train loss:0.428 acc:84.39 | val loss:0.355 acc:86.81\n",
      "c: 0.821272075176239\n",
      "c_high: 0.04302002117037773\n",
      "Epoch:0056 train loss:0.415 acc:84.75 | val loss:0.353 acc:86.81\n",
      "c: 0.8226121068000793\n",
      "c_high: 0.04153112694621086\n",
      "Epoch:0057 train loss:0.416 acc:85.08 | val loss:0.352 acc:86.88\n",
      "c: 0.8238509297370911\n",
      "c_high: 0.039999693632125854\n",
      "Epoch:0058 train loss:0.414 acc:85.00 | val loss:0.350 acc:86.93\n",
      "c: 0.8249620199203491\n",
      "c_high: 0.03871979936957359\n",
      "Epoch:0059 train loss:0.416 acc:84.59 | val loss:0.349 acc:86.89\n",
      "c: 0.8259496688842773\n",
      "c_high: 0.03773615136742592\n",
      "Epoch:0060 train loss:0.409 acc:85.32 | val loss:0.357 acc:86.72\n",
      "c: 0.8265889286994934\n",
      "c_high: 0.03905939683318138\n",
      "Epoch:0061 train loss:0.417 acc:85.60 | val loss:0.346 acc:87.18\n",
      "c: 0.8271114826202393\n",
      "c_high: 0.04072767123579979\n",
      "Epoch:0062 train loss:0.413 acc:85.04 | val loss:0.346 acc:87.24\n",
      "c: 0.8275865912437439\n",
      "c_high: 0.042231861501932144\n",
      "Epoch:0063 train loss:0.404 acc:85.41 | val loss:0.346 acc:87.23\n",
      "c: 0.8280930519104004\n",
      "c_high: 0.0430193692445755\n",
      "Epoch:0064 train loss:0.409 acc:85.28 | val loss:0.346 acc:87.18\n",
      "c: 0.8286252021789551\n",
      "c_high: 0.04319008067250252\n",
      "Epoch:0065 train loss:0.400 acc:85.87 | val loss:0.344 acc:87.54\n",
      "c: 0.8291980028152466\n",
      "c_high: 0.04267789050936699\n",
      "Epoch:0066 train loss:0.401 acc:85.42 | val loss:0.341 acc:87.73\n",
      "c: 0.8298190236091614\n",
      "c_high: 0.04145824536681175\n",
      "Epoch:0067 train loss:0.403 acc:85.55 | val loss:0.338 acc:87.99\n",
      "c: 0.8304493427276611\n",
      "c_high: 0.039838992059230804\n",
      "Epoch:0068 train loss:0.398 acc:85.81 | val loss:0.335 acc:87.96\n",
      "c: 0.8310704827308655\n",
      "c_high: 0.03797595202922821\n",
      "Epoch:0069 train loss:0.404 acc:85.27 | val loss:0.333 acc:87.77\n",
      "c: 0.8316421508789062\n",
      "c_high: 0.03621358424425125\n",
      "Epoch:0070 train loss:0.395 acc:85.50 | val loss:0.332 acc:87.84\n",
      "c: 0.8321645259857178\n",
      "c_high: 0.034577906131744385\n",
      "Epoch:0071 train loss:0.387 acc:86.00 | val loss:0.330 acc:87.80\n",
      "c: 0.8326053023338318\n",
      "c_high: 0.03339432179927826\n",
      "Epoch:0072 train loss:0.391 acc:85.78 | val loss:0.329 acc:87.78\n",
      "c: 0.8329617381095886\n",
      "c_high: 0.03272935375571251\n",
      "Epoch:0073 train loss:0.394 acc:85.74 | val loss:0.328 acc:87.81\n",
      "c: 0.8332615494728088\n",
      "c_high: 0.032349664717912674\n",
      "Epoch:0074 train loss:0.382 acc:86.17 | val loss:0.327 acc:87.83\n",
      "c: 0.8334597945213318\n",
      "c_high: 0.03272257745265961\n",
      "Epoch:0075 train loss:0.391 acc:85.93 | val loss:0.327 acc:87.81\n",
      "c: 0.833549976348877\n",
      "c_high: 0.033916670829057693\n",
      "Epoch:0076 train loss:0.393 acc:85.70 | val loss:0.326 acc:87.88\n",
      "c: 0.8335543274879456\n",
      "c_high: 0.03570772334933281\n",
      "Epoch:0077 train loss:0.389 acc:85.88 | val loss:0.324 acc:88.05\n",
      "c: 0.8335002660751343\n",
      "c_high: 0.03783402964472771\n",
      "Epoch:0078 train loss:0.382 acc:86.23 | val loss:0.322 acc:88.10\n",
      "c: 0.8334381580352783\n",
      "c_high: 0.039866991341114044\n",
      "Epoch:0079 train loss:0.378 acc:86.64 | val loss:0.322 acc:88.18\n",
      "c: 0.8333714604377747\n",
      "c_high: 0.04178808629512787\n",
      "Epoch:0080 train loss:0.387 acc:86.30 | val loss:0.321 acc:88.24\n",
      "c: 0.8333512544631958\n",
      "c_high: 0.043224457651376724\n",
      "Epoch:0081 train loss:0.388 acc:85.79 | val loss:0.322 acc:88.13\n",
      "c: 0.8333814740180969\n",
      "c_high: 0.04417162761092186\n",
      "Epoch:0082 train loss:0.383 acc:85.80 | val loss:0.322 acc:88.05\n",
      "c: 0.8334581851959229\n",
      "c_high: 0.04467783868312836\n",
      "Epoch:0083 train loss:0.375 acc:86.24 | val loss:0.321 acc:88.23\n",
      "c: 0.8335996866226196\n",
      "c_high: 0.04463112726807594\n",
      "Epoch:0084 train loss:0.376 acc:86.59 | val loss:0.319 acc:88.37\n",
      "c: 0.8337849378585815\n",
      "c_high: 0.04418808966875076\n",
      "Epoch:0085 train loss:0.383 acc:86.22 | val loss:0.318 acc:88.37\n",
      "c: 0.8339996933937073\n",
      "c_high: 0.043453898280858994\n",
      "Epoch:0086 train loss:0.374 acc:86.96 | val loss:0.318 acc:88.21\n",
      "c: 0.8342322111129761\n",
      "c_high: 0.04251469299197197\n",
      "Epoch:0087 train loss:0.369 acc:87.01 | val loss:0.318 acc:88.26\n",
      "c: 0.8343666195869446\n",
      "c_high: 0.04222029820084572\n",
      "Epoch:0088 train loss:0.378 acc:86.46 | val loss:0.316 acc:88.30\n",
      "c: 0.8344607949256897\n",
      "c_high: 0.042154520750045776\n",
      "Epoch:0089 train loss:0.373 acc:86.30 | val loss:0.315 acc:88.59\n",
      "c: 0.8345425724983215\n",
      "c_high: 0.04211868718266487\n",
      "Epoch:0090 train loss:0.373 acc:86.34 | val loss:0.314 acc:88.48\n",
      "c: 0.8346126675605774\n",
      "c_high: 0.04211294278502464\n",
      "Epoch:0091 train loss:0.368 acc:86.63 | val loss:0.317 acc:88.07\n",
      "c: 0.8346238732337952\n",
      "c_high: 0.042491309344768524\n",
      "Epoch:0092 train loss:0.369 acc:86.40 | val loss:0.317 acc:88.13\n",
      "c: 0.8345632553100586\n",
      "c_high: 0.043347910046577454\n",
      "Epoch:0093 train loss:0.367 acc:86.75 | val loss:0.313 acc:88.46\n",
      "c: 0.8344799876213074\n",
      "c_high: 0.04432442784309387\n",
      "Epoch:0094 train loss:0.362 acc:86.65 | val loss:0.313 acc:88.59\n",
      "c: 0.834419310092926\n",
      "c_high: 0.04510335996747017\n",
      "Epoch:0095 train loss:0.372 acc:86.55 | val loss:0.312 acc:88.48\n",
      "c: 0.8343657851219177\n",
      "c_high: 0.04579683020710945\n",
      "Epoch:0096 train loss:0.368 acc:86.37 | val loss:0.315 acc:88.18\n",
      "c: 0.8342365622520447\n",
      "c_high: 0.04696778953075409\n",
      "Epoch:0097 train loss:0.380 acc:86.48 | val loss:0.314 acc:88.38\n",
      "c: 0.8341066837310791\n",
      "c_high: 0.04811066761612892\n",
      "Epoch:0098 train loss:0.369 acc:86.38 | val loss:0.311 acc:88.75\n",
      "c: 0.8340307474136353\n",
      "c_high: 0.048875078558921814\n",
      "Epoch:0099 train loss:0.373 acc:86.67 | val loss:0.310 acc:88.70\n",
      "c: 0.8340254426002502\n",
      "c_high: 0.049163270741701126\n",
      "Epoch:0100 train loss:0.370 acc:86.52 | val loss:0.310 acc:88.81\n",
      "c: 0.834068238735199\n",
      "c_high: 0.049122754484415054\n",
      "Epoch:0101 train loss:0.371 acc:86.46 | val loss:0.311 acc:88.57\n",
      "c: 0.8341536521911621\n",
      "c_high: 0.04879052937030792\n",
      "Epoch:0102 train loss:0.365 acc:86.63 | val loss:0.311 acc:88.54\n",
      "c: 0.8342649340629578\n",
      "c_high: 0.04827278479933739\n",
      "Epoch:0103 train loss:0.362 acc:86.93 | val loss:0.310 acc:88.48\n",
      "c: 0.8344037532806396\n",
      "c_high: 0.04755856841802597\n",
      "Epoch:0104 train loss:0.360 acc:86.90 | val loss:0.310 acc:88.67\n",
      "c: 0.8344526290893555\n",
      "c_high: 0.047409143298864365\n",
      "Epoch:0105 train loss:0.360 acc:87.18 | val loss:0.309 acc:88.61\n",
      "c: 0.8344721794128418\n",
      "c_high: 0.047432221472263336\n",
      "Epoch:0106 train loss:0.361 acc:86.91 | val loss:0.309 acc:88.75\n"
     ]
    }
   ],
   "source": [
    "!python -u full-supervised.py --data pubmed --layer 64 --weight_decay 5e-6 --variant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
